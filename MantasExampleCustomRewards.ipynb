{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-E6bHhPyv8Um",
    "outputId": "6e6d721f-c88f-418c-cd96-9bf63414e556"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting stable-baselines3\n",
      "  Downloading stable_baselines3-2.3.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting gymnasium<0.30,>=0.28.1 (from stable-baselines3)\n",
      "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (1.26.4)\n",
      "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.4.1+cu121)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.2.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.2.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (3.7.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (4.12.2)\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium<0.30,>=0.28.1->stable-baselines3)\n",
      "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.16.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (2024.6.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable-baselines3) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3) (1.3.0)\n",
      "Downloading stable_baselines3-2.3.2-py3-none-any.whl (182 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m182.3/182.3 kB\u001B[0m \u001B[31m3.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m953.9/953.9 kB\u001B[0m \u001B[31m24.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Installing collected packages: farama-notifications, gymnasium, stable-baselines3\n",
      "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1 stable-baselines3-2.3.2\n",
      "Collecting metadrive-simulator\n",
      "  Downloading metadrive_simulator-0.4.2.3-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from metadrive-simulator) (2.32.3)\n",
      "Collecting gymnasium<0.29,>=0.28 (from metadrive-simulator)\n",
      "  Downloading gymnasium-0.28.1-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting numpy<=1.24.2,>=1.21.6 (from metadrive-simulator)\n",
      "  Downloading numpy-1.24.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from metadrive-simulator) (3.7.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from metadrive-simulator) (2.2.2)\n",
      "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from metadrive-simulator) (2.6.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from metadrive-simulator) (4.66.5)\n",
      "Collecting yapf (from metadrive-simulator)\n",
      "  Downloading yapf-0.40.2-py3-none-any.whl.metadata (45 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m45.4/45.4 kB\u001B[0m \u001B[31m2.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from metadrive-simulator) (0.13.1)\n",
      "Collecting progressbar (from metadrive-simulator)\n",
      "  Downloading progressbar-2.5.tar.gz (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Collecting panda3d==1.10.13 (from metadrive-simulator)\n",
      "  Downloading panda3d-1.10.13-cp310-cp310-manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting panda3d-gltf==0.13 (from metadrive-simulator)\n",
      "  Downloading panda3d_gltf-0.13-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from metadrive-simulator) (10.4.0)\n",
      "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from metadrive-simulator) (7.4.4)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from metadrive-simulator) (4.10.0.84)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from metadrive-simulator) (4.9.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from metadrive-simulator) (1.13.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from metadrive-simulator) (5.9.5)\n",
      "Requirement already satisfied: geopandas in /usr/local/lib/python3.10/dist-packages (from metadrive-simulator) (1.0.1)\n",
      "Requirement already satisfied: shapely in /usr/local/lib/python3.10/dist-packages (from metadrive-simulator) (2.0.6)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from metadrive-simulator) (3.16.1)\n",
      "Requirement already satisfied: Pygments in /usr/local/lib/python3.10/dist-packages (from metadrive-simulator) (2.18.0)\n",
      "Collecting panda3d-simplepbr>=0.6 (from panda3d-gltf==0.13->metadrive-simulator)\n",
      "  Downloading panda3d_simplepbr-0.12.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting jax-jumpy>=1.0.0 (from gymnasium<0.29,>=0.28->metadrive-simulator)\n",
      "  Downloading jax_jumpy-1.0.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.29,>=0.28->metadrive-simulator) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.29,>=0.28->metadrive-simulator) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.29,>=0.28->metadrive-simulator) (0.0.4)\n",
      "Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from geopandas->metadrive-simulator) (0.10.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from geopandas->metadrive-simulator) (24.1)\n",
      "Requirement already satisfied: pyproj>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from geopandas->metadrive-simulator) (3.6.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->metadrive-simulator) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->metadrive-simulator) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->metadrive-simulator) (2024.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->metadrive-simulator) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->metadrive-simulator) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->metadrive-simulator) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->metadrive-simulator) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->metadrive-simulator) (3.1.4)\n",
      "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->metadrive-simulator) (2.0.0)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->metadrive-simulator) (1.5.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->metadrive-simulator) (1.2.2)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->metadrive-simulator) (2.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->metadrive-simulator) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->metadrive-simulator) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->metadrive-simulator) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->metadrive-simulator) (2024.8.30)\n",
      "Requirement already satisfied: importlib-metadata>=6.6.0 in /usr/local/lib/python3.10/dist-packages (from yapf->metadrive-simulator) (8.4.0)\n",
      "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from yapf->metadrive-simulator) (4.3.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.6.0->yapf->metadrive-simulator) (3.20.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->metadrive-simulator) (1.16.0)\n",
      "Downloading metadrive_simulator-0.4.2.3-py3-none-any.whl (56.5 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m56.5/56.5 MB\u001B[0m \u001B[31m6.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading panda3d-1.10.13-cp310-cp310-manylinux2014_x86_64.whl (54.8 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m54.8/54.8 MB\u001B[0m \u001B[31m11.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading panda3d_gltf-0.13-py3-none-any.whl (25 kB)\n",
      "Downloading gymnasium-0.28.1-py3-none-any.whl (925 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m925.5/925.5 kB\u001B[0m \u001B[31m55.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading numpy-1.24.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m17.3/17.3 MB\u001B[0m \u001B[31m39.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading yapf-0.40.2-py3-none-any.whl (254 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m254.7/254.7 kB\u001B[0m \u001B[31m22.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading jax_jumpy-1.0.0-py3-none-any.whl (20 kB)\n",
      "Downloading panda3d_simplepbr-0.12.0-py3-none-any.whl (2.5 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.5/2.5 MB\u001B[0m \u001B[31m80.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hBuilding wheels for collected packages: progressbar\n",
      "  Building wheel for progressbar (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for progressbar: filename=progressbar-2.5-py3-none-any.whl size=12066 sha256=1dabb7a51f10baa47ac48fdb42aadd16619dcf79d8a7980c6d323bb8bbaa9701\n",
      "  Stored in directory: /root/.cache/pip/wheels/cd/17/e5/765d1a3112ff3978f70223502f6047e06c43a24d7c5f8ff95b\n",
      "Successfully built progressbar\n",
      "Installing collected packages: progressbar, panda3d, panda3d-simplepbr, numpy, yapf, panda3d-gltf, jax-jumpy, gymnasium, metadrive-simulator\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: gymnasium\n",
      "    Found existing installation: gymnasium 0.29.1\n",
      "    Uninstalling gymnasium-0.29.1:\n",
      "      Successfully uninstalled gymnasium-0.29.1\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "albumentations 1.4.15 requires numpy>=1.24.4, but you have numpy 1.24.2 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed gymnasium-0.28.1 jax-jumpy-1.0.0 metadrive-simulator-0.4.2.3 numpy-1.24.2 panda3d-1.10.13 panda3d-gltf-0.13 panda3d-simplepbr-0.12.0 progressbar-2.5 yapf-0.40.2\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "numpy"
        ]
       },
       "id": "34e02f6dbc5a4ec094ac5202e351834a"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "!pip install stable-baselines3\n",
    "!pip install metadrive-simulator"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir ./ppo_metadrive_tensorboard/"
   ],
   "metadata": {
    "id": "YItjXOhr2xBj"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Custom reward example"
   ],
   "metadata": {
    "id": "UQWKpoM_katY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Import necessary libraries\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from metadrive import MetaDriveEnv\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from functools import partial\n",
    "\n",
    "# --- Custom Environment Class ---\n",
    "class CustomMetaDriveEnv(MetaDriveEnv):\n",
    "    @classmethod\n",
    "    def default_config(cls):\n",
    "        config = MetaDriveEnv.default_config()\n",
    "        # Disable image observations and rendering\n",
    "        config.update({\n",
    "            \"image_observation\": False,\n",
    "            \"use_render\": False,\n",
    "            # \"num_agents\": 1,\n",
    "            # \"is_multi_agent\": False,\n",
    "            # \"manual_control\": False,\n",
    "            # \"agent_policy\": None,\n",
    "            # Other necessary configurations\n",
    "        })\n",
    "        # Configure vehicle sensors\n",
    "        config[\"vehicle_config\"].update({\n",
    "            \"lidar\": {\n",
    "                \"num_lasers\": 72,\n",
    "                \"distance\": 50,\n",
    "                \"num_others\": 0,\n",
    "            },\n",
    "            \"side_detector\": {\"num_lasers\": 0},\n",
    "            \"lane_line_detector\": {\"num_lasers\": 0},\n",
    "            \"use_navigation\": True,\n",
    "            \"max_speed\": 20.0,\n",
    "        }, allow_add_new_key=True)\n",
    "        return config\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        if config is None:\n",
    "            config = self.default_config()\n",
    "        super(CustomMetaDriveEnv, self).__init__(config)\n",
    "        self.speed_limit = self.config[\"vehicle_config\"][\"max_speed\"]\n",
    "\n",
    "    def reset(self, **kwargs): # Add **kwargs to accept additional keyword arguments\n",
    "        # Ensure that the agent is properly initialized\n",
    "        obs = super(CustomMetaDriveEnv, self).reset(**kwargs) # Pass kwargs to the superclass reset() method\n",
    "        if len(self.engine.agents) == 0:\n",
    "            self.setup_engine()\n",
    "            self.engine.setup_world()\n",
    "            self._add_agents()\n",
    "        return obs\n",
    "\n",
    "    def _add_agents(self):\n",
    "        # Add a single agent to the environment\n",
    "        self.default_agent = self.spawn_object(\n",
    "            self.config[\"vehicle_config\"][\"vehicle_class\"],\n",
    "            vehicle_config=self.config[\"vehicle_config\"]\n",
    "        )\n",
    "        self.agent_ids = [\"default_agent\"]\n",
    "        self.engine.agents[\"default_agent\"] = self.default_agent\n",
    "\n",
    "    def reward_function(self, vehicle_id: str):\n",
    "        # Get the default reward and reward_info from the base class\n",
    "        default_reward, reward_info = super(CustomMetaDriveEnv, self).reward_function(vehicle_id)\n",
    "        total_reward = default_reward\n",
    "        vehicle = self.engine.agents[vehicle_id]\n",
    "\n",
    "        # --- Speed Limit Penalty ---\n",
    "        speed_penalty = 0.0\n",
    "        if vehicle.speed > self.speed_limit:\n",
    "            speed_penalty = - (vehicle.speed - self.speed_limit) * 0.1\n",
    "            total_reward += speed_penalty\n",
    "\n",
    "        # --- Lane Keeping Reward ---\n",
    "        lane_reward = 0.0\n",
    "        if hasattr(vehicle, 'navigation') and vehicle.navigation.current_ref_lanes:\n",
    "            target_lane = vehicle.navigation.current_ref_lanes[0]\n",
    "            if vehicle.lane == target_lane:\n",
    "                lane_reward = 0.05\n",
    "                total_reward += lane_reward\n",
    "\n",
    "        # Update reward_info with custom information\n",
    "        reward_info['speed_penalty'] = speed_penalty\n",
    "        reward_info['lane_reward'] = lane_reward\n",
    "\n",
    "        return total_reward, reward_info\n",
    "\n",
    "# --- Environment Creation Function ---\n",
    "def create_env(seed=0):\n",
    "    env = CustomMetaDriveEnv({\n",
    "        # Map Configuration:\n",
    "        \"map\": \"Cr\",  # Use a predefined map\n",
    "        # Traffic Configuration:\n",
    "        # \"traffic_density\": 0.1,\n",
    "        # # Accident Probability:\n",
    "        # \"accident_prob\": 0,\n",
    "        # # Episode Horizon:\n",
    "        # \"horizon\": 1000,\n",
    "        # # Spawn Configuration:\n",
    "        # \"random_spawn_lane_index\": False,\n",
    "        # # Seed Configuration:\n",
    "        # \"start_seed\": seed,\n",
    "        # # Traffic Mode:\n",
    "        # \"traffic_mode\": \"trigger\",\n",
    "        # # Environment Variety:\n",
    "        # \"num_scenarios\": 100,\n",
    "        # Agent Configuration:\n",
    "        # \"num_agents\": 1,\n",
    "        # \"is_multi_agent\": False,\n",
    "        # \"manual_control\": False,\n",
    "        # \"agent_policy\": None,\n",
    "    })\n",
    "    return Monitor(env)\n",
    "\n",
    "# --- Set Random Seed for Reproducibility ---\n",
    "set_random_seed(0)\n",
    "\n",
    "# --- Number of Parallel Environments ---\n",
    "num_envs = 1  # Since we're using DummyVecEnv\n",
    "\n",
    "steps = 2048  # Number of steps per environment per update\n",
    "\n",
    "# --- Create the Vectorized Environment ---\n",
    "train_env = DummyVecEnv([partial(create_env) for i in range(num_envs)])\n",
    "\n",
    "# --- Verify the Observation Space ---\n",
    "obs = train_env.reset()\n",
    "print(\"Observation shape:\", obs.shape)\n",
    "print(\"Observation space:\", train_env.observation_space)\n",
    "print(\"Observation dtype:\", obs.dtype)\n",
    "\n",
    "# --- Define Policy Keyword Arguments with Shared Layers ---\n",
    "policy_kwargs = dict(\n",
    "    net_arch=[\n",
    "        {\"shared\": [256, 256]},\n",
    "        dict(\n",
    "            pi=[128, 64],  # Policy head with layers of 128 and 64 units\n",
    "            vf=[128, 64]   # Value head with layers of 128 and 64 units\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --- Instantiate the PPO Model ---\n",
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=train_env,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    n_steps=steps,\n",
    "    batch_size=steps * num_envs // 2,  # Keeping batch_size = steps * num_envs // 2\n",
    "    learning_rate=3e-4,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    ent_coef=0.0,\n",
    "    clip_range=0.2,\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./ppo_metadrive_tensorboard/\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ")\n",
    "\n",
    "# --- Train the Agent ---\n",
    "model.learn(total_timesteps=1_000_000)\n",
    "\n",
    "# --- Save the Trained Model ---\n",
    "model.save(\"ppo_metadrive_agent\")\n",
    "\n",
    "# --- Close the Environment ---\n",
    "train_env.close()\n"
   ],
   "metadata": {
    "id": "OoZCeNPNkaGg",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "f022e983-6f54-4007-84d6-3f9e7c76e1df",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# train_env.close()\n"
   ],
   "metadata": {
    "id": "wcLK4CIcjeHn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "###Advanced Exploration Techniques (not yet working)"
   ],
   "metadata": {
    "id": "4nUGiheyAraZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# pip install --upgrade gymnasium\n"
   ],
   "metadata": {
    "id": "ieNCbp4EJES_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# # Import necessary libraries\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import gym\n",
    "# import gymnasium as gymn\n",
    "# from stable_baselines3 import PPO\n",
    "# from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "# from stable_baselines3.common.monitor import Monitor\n",
    "# from stable_baselines3.common.utils import set_random_seed\n",
    "# from stable_baselines3.common.callbacks import BaseCallback\n",
    "# from functools import partial\n",
    "# import warnings\n",
    "\n",
    "# from metadrive import MetaDriveEnv\n",
    "\n",
    "# # Suppress potential warnings for cleaner output\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# # --- Define RND Module ---\n",
    "# class RNDModule(nn.Module):\n",
    "#     def __init__(self, obs_dim, hidden_size=256, device='cpu'):\n",
    "#         super(RNDModule, self).__init__()\n",
    "#         self.device = device\n",
    "#         # Fixed Target Network\n",
    "#         self.target = nn.Sequential(\n",
    "#             nn.Linear(obs_dim, hidden_size),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_size, hidden_size),\n",
    "#             nn.ReLU()\n",
    "#         ).to(self.device)\n",
    "#         for param in self.target.parameters():\n",
    "#             param.requires_grad = False  # Freeze target network\n",
    "\n",
    "#         # Predictor Network\n",
    "#         self.predictor = nn.Sequential(\n",
    "#             nn.Linear(obs_dim, hidden_size),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_size, hidden_size),\n",
    "#             nn.ReLU()\n",
    "#         ).to(self.device)\n",
    "\n",
    "#     def forward(self, state):\n",
    "#         state = state.to(self.device)\n",
    "#         target_features = self.target(state)\n",
    "#         pred_features = self.predictor(state)\n",
    "#         return pred_features, target_features\n",
    "\n",
    "# # --- Define Intrinsic Reward Callback ---\n",
    "# class IntrinsicRewardCallback(BaseCallback):\n",
    "#     \"\"\"\n",
    "#     Custom callback for logging intrinsic and total rewards.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, verbose=0):\n",
    "#         super(IntrinsicRewardCallback, self).__init__(verbose)\n",
    "\n",
    "#     def _on_step(self) -> bool:\n",
    "#         # Access the last infos\n",
    "#         for info in self.locals.get('infos', []):\n",
    "#             if 'intrinsic_reward' in info:\n",
    "#                 self.logger.record('intrinsic_reward', info['intrinsic_reward'])\n",
    "#             if 'total_reward' in info:\n",
    "#                 self.logger.record('total_reward', info['total_reward'])\n",
    "#         return True\n",
    "\n",
    "# # --- Define Custom Wrapper ---\n",
    "# class GymnasiumToGymWrapper(gym.Env):\n",
    "#     \"\"\"\n",
    "#     A custom wrapper to convert a Gymnasium environment to a Gym-compatible environment.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, gymnasium_env):\n",
    "#         super(GymnasiumToGymWrapper, self).__init__()\n",
    "#         self.gymn_env = gymnasium_env\n",
    "\n",
    "#         # Convert observation_space from gymnasium to gym\n",
    "#         if isinstance(self.gymn_env.observation_space, gymn.spaces.Box):\n",
    "#             self.observation_space = gym.spaces.Box(\n",
    "#                 low=self.gymn_env.observation_space.low,\n",
    "#                 high=self.gymn_env.observation_space.high,\n",
    "#                 dtype=self.gymn_env.observation_space.dtype\n",
    "#             )\n",
    "#         else:\n",
    "#             raise NotImplementedError(\"Only Box observation spaces are supported.\")\n",
    "\n",
    "#         # Convert action_space from gymnasium to gym\n",
    "#         if isinstance(self.gymn_env.action_space, gymn.spaces.Discrete):\n",
    "#             self.action_space = gym.spaces.Discrete(self.gymn_env.action_space.n)\n",
    "#         elif isinstance(self.gymn_env.action_space, gymn.spaces.Box):\n",
    "#             self.action_space = gym.spaces.Box(\n",
    "#                 low=self.gymn_env.action_space.low,\n",
    "#                 high=self.gymn_env.action_space.high,\n",
    "#                 dtype=self.gymn_env.action_space.dtype\n",
    "#             )\n",
    "#         else:\n",
    "#             raise NotImplementedError(\"Only Discrete and Box action spaces are supported.\")\n",
    "\n",
    "#     def reset(self, **kwargs):\n",
    "#         \"\"\"\n",
    "#         Resets the environment and returns the initial observation.\n",
    "#         \"\"\"\n",
    "#         obs, info = self.gymn_env.reset(**kwargs)\n",
    "#         return obs\n",
    "\n",
    "#     def step(self, action):\n",
    "#         \"\"\"\n",
    "#         Takes an action and returns the next observation, reward, done, and info.\n",
    "#         \"\"\"\n",
    "#         obs, reward, terminated, truncated, info = self.gymn_env.step(action)\n",
    "#         done = terminated or truncated\n",
    "#         return obs, reward, done, info\n",
    "\n",
    "#     def render(self, mode='human'):\n",
    "#         \"\"\"\n",
    "#         Renders the environment.\n",
    "#         \"\"\"\n",
    "#         return self.gymn_env.render(mode=mode)\n",
    "\n",
    "#     def close(self):\n",
    "#         \"\"\"\n",
    "#         Closes the environment.\n",
    "#         \"\"\"\n",
    "#         self.gymn_env.close()\n",
    "\n",
    "# # --- Define RND Wrapper ---\n",
    "# class RNDWrapper(gymn.Env):\n",
    "#     \"\"\"\n",
    "#     A custom wrapper to integrate Random Network Distillation (RND) intrinsic rewards.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, gymnasium_env, alpha=0.1, hidden_size=256, device='cpu'):\n",
    "#         super(RNDWrapper, self).__init__()\n",
    "#         self.gymn_env = gymnasium_env\n",
    "#         self.alpha = alpha\n",
    "#         self.device = torch.device(device)\n",
    "#         self.obs_dim = int(np.prod(self.gymn_env.observation_space.shape))\n",
    "#         self.rnd = RNDModule(obs_dim=self.obs_dim, hidden_size=hidden_size, device=self.device)\n",
    "#         self.rnd.to(self.device)\n",
    "#         self.rnd_optimizer = optim.Adam(self.rnd.predictor.parameters(), lr=1e-3)\n",
    "\n",
    "#     def reset(self, **kwargs):\n",
    "#         \"\"\"\n",
    "#         Resets the environment and returns the initial observation.\n",
    "#         \"\"\"\n",
    "#         obs, info = self.gymn_env.reset(**kwargs)\n",
    "#         return obs\n",
    "\n",
    "#     def step(self, action):\n",
    "#         \"\"\"\n",
    "#         Takes an action and returns the next observation, reward, done, and info.\n",
    "#         \"\"\"\n",
    "#         obs, reward, terminated, truncated, info = self.gymn_env.step(action)\n",
    "#         done = terminated or truncated\n",
    "\n",
    "#         # Convert observation to tensor\n",
    "#         state_tensor = torch.tensor(obs, dtype=torch.float32).to(self.device)\n",
    "#         with torch.no_grad():\n",
    "#             _, target_features = self.rnd(state_tensor)\n",
    "#         pred_features, _ = self.rnd(state_tensor)\n",
    "\n",
    "#         # Calculate intrinsic reward\n",
    "#         intrinsic_reward = nn.MSELoss()(pred_features, target_features.detach())\n",
    "#         intrinsic_reward = intrinsic_reward.item() * self.alpha\n",
    "\n",
    "#         # Update total reward\n",
    "#         total_reward = reward + intrinsic_reward\n",
    "\n",
    "#         # Add intrinsic reward to info\n",
    "#         info['intrinsic_reward'] = intrinsic_reward\n",
    "#         info['total_reward'] = total_reward\n",
    "\n",
    "#         # Compute loss and optimize predictor\n",
    "#         rnd_loss = nn.MSELoss()(pred_features, target_features.detach())\n",
    "#         self.rnd_optimizer.zero_grad()\n",
    "#         rnd_loss.backward()\n",
    "#         self.rnd_optimizer.step()\n",
    "\n",
    "#         return obs, total_reward, done, info\n",
    "\n",
    "#     def render(self, mode='human'):\n",
    "#         \"\"\"\n",
    "#         Renders the environment.\n",
    "#         \"\"\"\n",
    "#         return self.gymn_env.render(mode=mode)\n",
    "\n",
    "#     def close(self):\n",
    "#         \"\"\"\n",
    "#         Closes the environment.\n",
    "#         \"\"\"\n",
    "#         self.gymn_env.close()\n",
    "\n",
    "# # --- Environment Creation Function ---\n",
    "# def create_env_with_rnd(seed=0):\n",
    "#     \"\"\"\n",
    "#     Creates the MetaDrive environment wrapped with RND and GymnasiumToGymWrapper.\n",
    "#     \"\"\"\n",
    "#     # Initialize the custom MetaDrive environment\n",
    "#     env = MetaDriveEnv({\n",
    "#         # Map Configuration:\n",
    "#         \"map\": \"Cr\",  # Use a predefined map\n",
    "#         # Traffic Configuration:\n",
    "#         \"traffic_density\": 0.1,\n",
    "#         # Accident Probability:\n",
    "#         \"accident_prob\": 0,\n",
    "#         # Episode Horizon:\n",
    "#         \"horizon\": 1000,\n",
    "#         # Spawn Configuration:\n",
    "#         \"random_spawn_lane_index\": False,\n",
    "#         # Seed Configuration:\n",
    "#         \"start_seed\": seed,\n",
    "#         # Traffic Mode:\n",
    "#         \"traffic_mode\": \"trigger\",\n",
    "#         # Environment Variety:\n",
    "#         \"num_scenarios\": 100,\n",
    "#         # Vehicle Configuration:\n",
    "#         \"vehicle_config\": {\n",
    "#             \"lidar\": {\n",
    "#                 \"num_lasers\": 72,\n",
    "#                 \"distance\": 50,\n",
    "#                 \"num_others\": 0,\n",
    "#             },\n",
    "#             \"side_detector\": {\"num_lasers\": 0},\n",
    "#             \"lane_line_detector\": {\"num_lasers\": 0},\n",
    "#             # \"max_speed\": 20.0,\n",
    "#         },\n",
    "#         # Additional configurations if necessary\n",
    "#     })\n",
    "\n",
    "#     # Wrap with RNDWrapper\n",
    "#     env = RNDWrapper(env, alpha=0.1, hidden_size=256, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#     # Wrap with GymnasiumToGymWrapper\n",
    "#     env = GymnasiumToGymWrapper(env)\n",
    "\n",
    "#     # Optionally, wrap with Monitor for logging\n",
    "#     env = Monitor(env)\n",
    "\n",
    "#     return env\n",
    "\n",
    "# # --- Set Random Seed for Reproducibility ---\n",
    "# set_random_seed(0)\n",
    "\n",
    "# # --- Number of Parallel Environments ---\n",
    "# num_envs = 1  # Adjust based on your system's capacity\n",
    "\n",
    "# steps = 2048  # Number of steps per environment per update\n",
    "\n",
    "# # --- Create the Vectorized Environment ---\n",
    "# try:\n",
    "#     train_env = DummyVecEnv([partial(create_env_with_rnd, seed=i) for i in range(num_envs)])\n",
    "#     print(\"Vectorized environment created successfully.\")\n",
    "# except NotImplementedError as e:\n",
    "#     print(\"Failed to create vectorized environment:\", e)\n",
    "#     # Optionally, handle the error or exit\n",
    "#     raise e\n",
    "\n",
    "# # --- Verify the Observation Space ---\n",
    "# obs = train_env.reset()\n",
    "# print(\"Observation shape:\", obs.shape)\n",
    "# print(\"Observation space:\", train_env.observation_space)\n",
    "# print(\"Observation dtype:\", obs.dtype)\n",
    "\n",
    "# # --- Define Policy Keyword Arguments (Using Default Architecture) ---\n",
    "# policy_kwargs = dict(\n",
    "#     net_arch=[256, 256, dict(pi=[128], vf=[128])],\n",
    "# )\n",
    "\n",
    "# # --- Instantiate the PPO Model ---\n",
    "# model = PPO(\n",
    "#     policy=\"MlpPolicy\",\n",
    "#     env=train_env,\n",
    "#     policy_kwargs=policy_kwargs,  # Use your custom policy architecture\n",
    "#     n_steps=steps,\n",
    "#     batch_size=steps * num_envs // 2,  # Keeping batch_size = steps * num_envs // 2\n",
    "#     learning_rate=3e-4,\n",
    "#     gamma=0.99,\n",
    "#     gae_lambda=0.95,\n",
    "#     ent_coef=0.01,  # Increased entropy coefficient to encourage exploration\n",
    "#     clip_range=0.2,\n",
    "#     verbose=1,\n",
    "#     tensorboard_log=\"./ppo_metadrive_tensorboard/\",\n",
    "#     device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "# )\n",
    "\n",
    "# # --- Instantiate the Callback ---\n",
    "# intrinsic_reward_callback = IntrinsicRewardCallback()\n",
    "\n",
    "# # --- Train the Agent ---\n",
    "# print(\"Starting training...\")\n",
    "# try:\n",
    "#     model.learn(total_timesteps=100_000, callback=intrinsic_reward_callback)  # Start with 100k\n",
    "#     print(\"Initial training phase completed.\")\n",
    "#     model.learn(total_timesteps=900_000, callback=intrinsic_reward_callback)  # Continue to 1M\n",
    "#     print(\"Training completed.\")\n",
    "# except Exception as e:\n",
    "#     print(\"An error occurred during training:\", e)\n",
    "\n",
    "# # --- Save the Trained Model ---\n",
    "# try:\n",
    "#     model.save(\"ppo_metadrive_agent_with_rnd\")\n",
    "#     print(\"Model saved as 'ppo_metadrive_agent_with_rnd'.\")\n",
    "# except Exception as e:\n",
    "#     print(\"Failed to save the model:\", e)\n",
    "\n",
    "# # --- Close the Environment ---\n",
    "# try:\n",
    "#     train_env.close()\n",
    "#     print(\"Environment closed.\")\n",
    "# except Exception as e:\n",
    "#     print(\"Failed to close the environment:\", e)\n"
   ],
   "metadata": {
    "id": "w1xuGP6ABIxq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# train_env.close()\n"
   ],
   "metadata": {
    "id": "Mq42MxJtbSLI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "###Video download if trying on google collab"
   ],
   "metadata": {
    "id": "YAEu7t_Bwlaf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# from stable_baselines3.common.vec_env import VecVideoRecorder\n",
    "\n",
    "# # Record a video of the agent’s performance\n",
    "# video_length = 1000  # Number of steps to record\n",
    "\n",
    "# eval_env = DummyVecEnv([create_env])\n",
    "# eval_env = VecVideoRecorder(\n",
    "#     eval_env, \"videos/\", record_video_trigger=lambda x: x == 0, video_length=video_length\n",
    "# )\n",
    "\n",
    "# obs = eval_env.reset()\n",
    "# for _ in range(video_length):\n",
    "#     action, _states = model.predict(obs, deterministic=True)\n",
    "#     obs, rewards, dones, info = eval_env.step(action)\n",
    "\n",
    "# eval_env.close()\n"
   ],
   "metadata": {
    "id": "gkPZEI9Iyaeg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "###Evaluate"
   ],
   "metadata": {
    "id": "0-5kwhQ-2VVw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# # Create a single evaluation environment\n",
    "# eval_env = MetaDriveEnv({\n",
    "#     \"use_render\": True,  # Enable rendering for visual evaluation\n",
    "#     \"map\": \"C\",\n",
    "#     \"traffic_density\": 0,\n",
    "#     \"accident_prob\": 0,\n",
    "#     \"horizon\": 500\n",
    "# })\n",
    "\n",
    "# # Reset the environment and evaluate the trained agent\n",
    "# obs = eval_env.reset()\n",
    "# for _ in range(1000):\n",
    "#     action, _ = model.predict(obs, deterministic=True)\n",
    "#     obs, reward, done, info = eval_env.step(action)\n",
    "#     eval_env.render()  # Render the environment for visual feedback\n",
    "#     if done:\n",
    "#         obs = eval_env.reset()\n",
    "\n",
    "# eval_env.close()\n"
   ],
   "metadata": {
    "id": "ZC-DPldd2X_K"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "###Model that takes into consideration time"
   ],
   "metadata": {
    "id": "J5rZmzo_LQte"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# pip install stable-baselines3[extra]\n",
    "# pip install sb3-contrib\n",
    "# from sb3_contrib import RecurrentPPO\n",
    "\n",
    "# # Use RecurrentPPO instead of PPO\n",
    "# model = RecurrentPPO(\n",
    "#     \"MlpLstmPolicy\",\n",
    "#     train_env,\n",
    "#     verbose=1,\n",
    "#     tensorboard_log=\"./ppo_metadrive_tensorboard/\",\n",
    "#     device=\"cuda\",\n",
    "#     n_steps=steps,\n",
    "#     batch_size=steps * num_envs,\n",
    "#     policy_kwargs=dict(\n",
    "#         lstm_hidden_size=256,  # Size of the LSTM hidden state\n",
    "#         net_arch=[dict(vf=[256], pi=[256])]  # Define policy and value networks\n",
    "#     )\n",
    "# )\n"
   ],
   "metadata": {
    "id": "Np3QRJ5-LU7l"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
